# ğŸš€ Triton BERT Text Classification with Server Latency

This project demonstrates how to deploy a **fine-tuned BERT text classification model** using **NVIDIA Triton Inference Server (Python Backend)**, with:

* âœ… Top-K class predictions
* âœ… Confidence scores
* âœ… **Pure server-side response time (ms)**
* âœ… Optional Prometheus metrics
* âœ… Docker-based deployment

---

## ğŸ“Œ Features

* **BERT-based text classifier**
* Supports **batch inference**
* Returns:

  * `CLASS_IDS`
  * `CLASS_NAMES`
  * `PROBS`
  * `SERVER_TIME_MS` (true server processing time)
* Fallback to **BERT MLM** if fine-tuned artifacts are missing
* Runs on **CPU or GPU**
* Compatible with **Windows + Docker Desktop**

---

## ğŸ“‚ Project Structure

```
Triton deployment of bert model/
â”‚
â”œâ”€â”€ model_repository/
â”‚   â””â”€â”€ classification/
â”‚       â”œâ”€â”€ 1/
â”‚       â”‚   â””â”€â”€ model.py
â”‚       â””â”€â”€ config.pbtxt
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ label_encoder.pkl
â”‚
â”œâ”€â”€ test2.py
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ§  Model Artifacts

Place the following files in the `dataset/` directory:

* `best_model.pt` â†’ fine-tuned BERT classifier weights
* `label_encoder.pkl` â†’ sklearn `LabelEncoder` used during training

These files are **mounted into the container** at runtime.

---

## âš™ï¸ Triton Model Configuration (`config.pbtxt`)

Ensure the following outputs are declared:

```protobuf
output {
  name: "LOGITS"
  data_type: TYPE_FP32
  dims: [ -1, -1 ]
}

output {
  name: "CLASS_IDS"
  data_type: TYPE_INT64
  dims: [ -1, -1 ]
}

output {
  name: "CLASS_NAMES"
  data_type: TYPE_STRING
  dims: [ -1, -1 ]
}

output {
  name: "PROBS"
  data_type: TYPE_FP32
  dims: [ -1, -1 ]
}

output {
  name: "SERVER_TIME_MS"
  data_type: TYPE_FP32
  dims: [ 1 ]
}
```

âš ï¸ `SERVER_TIME_MS` **must be declared**, otherwise the client will receive `None`.

---

## ğŸ³ Docker Build

Build the Triton image:

```bash
docker build -t bert-prediction-v1 .
```

---

## â–¶ï¸ Run Triton Server

```bash
docker run --gpus '"device=0"' --rm -it \
  -p 8000:8000 \
  -p 8001:8001 \
  -p 8002:8002 \
  -v "C:/Users/Tejas Kakade/OneDrive/Desktop/Triton deployment of bert model/model_repository:/models" \
  -v "C:/Users/Tejas Kakade/OneDrive/Desktop/Triton deployment of bert model/dataset:/artifacts" \
  bert-prediction-v1
```

### Ports

| Port | Purpose            |
| ---- | ------------------ |
| 8000 | HTTP inference     |
| 8001 | GRPC               |
| 8002 | Prometheus metrics |

---

## ğŸ§ª Client Testing

Run the client:

```bash
python test2.py
```

### Example Output

```
Enter text to classify: hello
Predicted Class: Greeting (ID: 1, Confidence: 80.03%) | Server Time: 12.41 ms
```

---

## â±ï¸ Server Response Time (Important)

* `SERVER_TIME_MS` measures **only server-side processing**
* Includes:

  * Tokenization
  * Model forward pass
  * Output generation
* Excludes:

  * Client overhead
  * Network latency

This timing is measured **inside `model.py`** using `time.perf_counter()`.

---

## ğŸ“Š Prometheus Metrics (Optional)

If enabled, metrics are available at:

```
http://localhost:8002/metrics
```

Key metrics:

* `text_classification_latency_seconds`
* `text_classification_requests_total`
* `text_classification_top1_confidence`

---

## ğŸ”„ Common Troubleshooting

### âŒ `SERVER_TIME_MS is None`

âœ” Ensure:

* Output is declared in `config.pbtxt`
* Triton container is restarted
* Client requests the output

---

### âŒ Model falls back to MLM

âœ” Check:

* `best_model.pt` exists
* `label_encoder.pkl` exists
* Correct volume mount path

---

### âŒ Triton fails to load model

âœ” Run:

```bash
docker logs <container_id>
```

Look for Python or indentation errors.

---

## ğŸ§¹ Cleanup Docker (Optional)

To fully reset Docker:

```bash
docker system prune -a --volumes
```

âš ï¸ This deletes all containers, images, and caches.

---

## ğŸ“Œ Tech Stack

* Python 3.10
* PyTorch
* Hugging Face Transformers
* NVIDIA Triton Inference Server
* Docker
* Prometheus (optional)

---

## âœ… Status

âœ” Production-ready
âœ” Server-side latency enabled
âœ” GPU compatible

---

If you want, I can also provide:

* ğŸ“ˆ Performance benchmarking
* ğŸ” GPU-only latency measurement
* ğŸ“¦ Minimal Triton config
* ğŸ§  Model optimization tips

Just let me know ğŸ‘
